{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b3676-75fa-4443-a288-35bf6c8dc0b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2147008  yolov8.nn.modules.head.Detect                [80, [128, 256, 512]]         \n",
      "YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients, 28.8 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/*': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:513: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels... 2836 i\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels... 556 imag\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/0/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/0/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.93G      1.998      3.115      1.952         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n",
      "\n",
      "1 epochs completed in 0.040 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/0/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/0/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/0/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.5ms preprocess, 45.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/0/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805       0.19      0.133     0.0847     0.0309\n",
      "          blunt_weapon        556        334       0.16      0.168     0.0982     0.0352\n",
      "                 knife        556        471       0.22     0.0977     0.0711     0.0266\n",
      "Speed: 0.3ms preprocess, 11.1ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/0/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.08465253070548769.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 10...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 4.3s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.08465253070548769.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (5.4s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.08465253070548769.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.08465253070548769.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/1/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/1/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.95G       2.47      3.354      2.333         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.052 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/1/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/1/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/1/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.5ms preprocess, 54.4ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/1/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.251      0.242      0.161     0.0614\n",
      "          blunt_weapon        556        334      0.167      0.257      0.114      0.043\n",
      "                 knife        556        471      0.335      0.227      0.208     0.0798\n",
      "Speed: 0.3ms preprocess, 15.3ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/1/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.16080421164794248.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 4.3s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.16080421164794248.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (5.4s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.16080421164794248.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.16080421164794248.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/2/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/2/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.95G      1.916      2.547      1.926         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.074 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/2/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/2/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/2/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.5ms preprocess, 55.3ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/2/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.329      0.334      0.264      0.105\n",
      "          blunt_weapon        556        334      0.268      0.308      0.216     0.0914\n",
      "                 knife        556        471      0.391      0.359      0.312      0.119\n",
      "Speed: 0.4ms preprocess, 27.1ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/2/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.2638412237545805.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 6.7s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.2638412237545805.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (68.7s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.2638412237545805.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.2638412237545805.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/3/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/3/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.95G      1.665      2.137      1.753         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.092 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/3/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/3/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/3/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 56.4ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/3/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.421      0.338      0.305      0.133\n",
      "          blunt_weapon        556        334      0.378      0.332      0.265      0.127\n",
      "                 knife        556        471      0.463      0.344      0.346       0.14\n",
      "Speed: 0.5ms preprocess, 29.2ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/3/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.30538724682226703.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 7.1s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.30538724682226703.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (8.6s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.30538724682226703.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.30538724682226703.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/4/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/4/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.99G      1.505      1.857      1.639         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.090 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/4/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/4/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/4/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 55.9ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/4/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805       0.35      0.371        0.3      0.122\n",
      "          blunt_weapon        556        334      0.383       0.26      0.265      0.117\n",
      "                 knife        556        471      0.316      0.482      0.336      0.127\n",
      "Speed: 0.8ms preprocess, 34.5ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/4/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.30014076131162604.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 7.0s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.30014076131162604.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (8.6s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.30014076131162604.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.30014076131162604.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/5/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/5/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.94G      1.387       1.64       1.55         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.088 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/5/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/5/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/5/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.8ms preprocess, 57.8ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/5/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.402      0.384       0.33      0.148\n",
      "          blunt_weapon        556        334      0.465      0.314      0.304      0.147\n",
      "                 knife        556        471      0.339      0.454      0.356       0.15\n",
      "Speed: 0.5ms preprocess, 33.5ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/5/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3300499930351825.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 4.9s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3300499930351825.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (6.1s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3300499930351825.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3300499930351825.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/6/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/6/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.97G      1.278      1.452      1.474         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.093 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/6/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/6/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/6/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.5ms preprocess, 59.2ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/6/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.437      0.381      0.333      0.148\n",
      "          blunt_weapon        556        334      0.482      0.296      0.307      0.144\n",
      "                 knife        556        471      0.393      0.465      0.358      0.151\n",
      "Speed: 0.6ms preprocess, 35.6ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/6/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3328834888180401.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 6.9s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3328834888180401.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (8.3s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3328834888180401.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3328834888180401.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/7/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/7/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.94G      1.206      1.332      1.417         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.090 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/7/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/7/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/7/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 56.8ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/7/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.419      0.415      0.379      0.171\n",
      "          blunt_weapon        556        334      0.447      0.341      0.336      0.168\n",
      "                 knife        556        471       0.39      0.488      0.422      0.174\n",
      "Speed: 0.6ms preprocess, 35.8ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/7/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.37903775815332846.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 5.3s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.37903775815332846.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (6.4s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.37903775815332846.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.37903775815332846.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/8/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/8/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.97G      1.124       1.22      1.363         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.100 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/8/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/8/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/8/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.6ms preprocess, 57.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/8/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.436      0.381      0.327      0.148\n",
      "          blunt_weapon        556        334      0.468      0.282      0.305      0.145\n",
      "                 knife        556        471      0.405       0.48      0.348       0.15\n",
      "Speed: 0.6ms preprocess, 41.9ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/8/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3267257834630103.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 4.8s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3267257834630103.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (66.4s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3267257834630103.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3267257834630103.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/9/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/9/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.95G      1.062       1.11      1.317         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.106 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/9/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/9/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/9/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.9ms preprocess, 59.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/9/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.421      0.424      0.358      0.155\n",
      "          blunt_weapon        556        334      0.494      0.332      0.316      0.151\n",
      "                 knife        556        471      0.349      0.516      0.399      0.159\n",
      "Speed: 0.5ms preprocess, 43.6ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/9/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3575238402946521.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 8.1s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3575238402946521.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (9.7s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3575238402946521.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3575238402946521.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/10/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/10/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.98G      1.013      1.045      1.284         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.104 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/10/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/10/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/10/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.6ms preprocess, 61.3ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/10/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.363      0.378      0.307      0.139\n",
      "          blunt_weapon        556        334      0.408      0.368      0.349      0.173\n",
      "                 knife        556        471      0.317      0.388      0.264      0.105\n",
      "Speed: 0.6ms preprocess, 40.1ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/10/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3065827735787723.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 5.8s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3065827735787723.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (7.1s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3065827735787723.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3065827735787723.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/11/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/11/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.95G      0.965     0.9754      1.251         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.108 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/11/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/11/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/11/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 59.7ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/11/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805      0.404       0.37      0.294       0.13\n",
      "          blunt_weapon        556        334      0.404      0.308       0.25      0.127\n",
      "                 knife        556        471      0.404      0.431      0.338      0.132\n",
      "Speed: 0.4ms preprocess, 15.7ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/11/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.29368113678749963.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 6.3s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.29368113678749963.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (7.7s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.29368113678749963.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.29368113678749963.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/12/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/12/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.96G     0.9132     0.9183      1.221         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.116 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/12/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/12/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/12/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 61.1ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/12/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805       0.37      0.412      0.313      0.135\n",
      "          blunt_weapon        556        334      0.346      0.371       0.28      0.134\n",
      "                 knife        556        471      0.394      0.452      0.346      0.136\n",
      "Speed: 0.7ms preprocess, 42.6ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/12/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3131379920289693.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 6.3s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3131379920289693.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (7.8s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3131379920289693.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.3131379920289693.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/13/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/13/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.97G     0.8718     0.8546      1.194         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.108 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/13/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/13/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/13/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.5ms preprocess, 60.0ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/13/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805       0.41      0.318      0.265      0.112\n",
      "          blunt_weapon        556        334      0.398      0.308      0.248      0.111\n",
      "                 knife        556        471      0.423      0.328      0.281      0.114\n",
      "Speed: 0.7ms preprocess, 40.5ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/13/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.26456598764141637.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (0.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 6.0s, saved as '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.26456598764141637.onnx' (42.6 MB)\n",
      "\n",
      "Export complete (12.2s)\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.26456598764141637.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/best_25_0.26456598764141637.onnx imgsz=640 data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml, epochs=1, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.01, iou=0.7, max_det=50, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=None, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/runs/detect/train\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  yolov8.nn.modules.conv.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  yolov8.nn.modules.conv.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  yolov8.nn.modules.block.C2f                  [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  yolov8.nn.modules.conv.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  yolov8.nn.modules.block.C2f                  [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  yolov8.nn.modules.conv.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  yolov8.nn.modules.block.C2f                  [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  yolov8.nn.modules.conv.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  yolov8.nn.modules.block.C2f                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  yolov8.nn.modules.block.SPPF                 [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 12                  -1  1    591360  yolov8.nn.modules.block.C2f                  [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 15                  -1  1    148224  yolov8.nn.modules.block.C2f                  [384, 128, 1]                 \n",
      " 16                  -1  1    147712  yolov8.nn.modules.conv.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 18                  -1  1    493056  yolov8.nn.modules.block.C2f                  [384, 256, 1]                 \n",
      " 19                  -1  1    590336  yolov8.nn.modules.conv.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolov8.nn.modules.conv.Concat                [1]                           \n",
      " 21                  -1  1   1969152  yolov8.nn.modules.block.C2f                  [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  yolov8.nn.modules.head.Detect                [2, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/labels.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_15_png_jpg.rf.8e762a776f609080e739a5aa5a3a59e0.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_17_png_jpg.rf.a199ac00747b83c5940d151ea55a9eea.jpg: 4 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/train/images/knife_7_34_png_jpg.rf.6bf53d87a0ed57645c61c99a96302744.jpg: 4 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 4025. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/14/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/14/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.96G     0.8395     0.8247      1.174         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.104 hours.\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/14/train/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/14/train/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/14/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        556        805          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 6.6ms preprocess, 63.0ms inference, 0.0ms loss, 11.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s/14/train\u001b[0m\n",
      "Ultralytics YOLOv8.0.216 ðŸš€ Python-3.8.10 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/labels.cache... 55\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/irfan/Desktop/Data/blunt.v2i.yolov8/valid/images/knife_12_29_png_jpg.rf.0b9e7d0b2e2031be6d00e6172e875194.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 13, len(boxes) = 805. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('./yolov8')\n",
    "from yolov8 import YOLO\n",
    "\n",
    "class ResultLogger():\n",
    "    def __init__(self,save_path='log.csv'):\n",
    "        self.log_df = log_df = pd.DataFrame()\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def update(self,model,val_results,epoch):\n",
    "        self.log_df.loc[epoch,'mAP50']= round(val_results.box.map50,4)\n",
    "        self.log_df.loc[epoch,'mAP75']= round(val_results.box.map75,4)\n",
    "        res = val_results.box\n",
    "        for ind in val_results.box.ap_class_index:\n",
    "            self.log_df.loc[epoch,model.names[ind]] = round(val_results.box.ap50[ind],4)\n",
    "        for ind in range(len(model.trainer.loss_names)):\n",
    "            self.log_df.loc[epoch,model.trainer.loss_names[ind]] = round(model.trainer.tloss[ind].cpu().numpy().item(),4)\n",
    "        self.log_df.to_csv(self.save_path)\n",
    "        \n",
    "# Load a model\n",
    "#model = YOLO('yolov8n.yaml')  # build a new model from YAML\n",
    "#model = YOLO('../models/yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "model = YOLO('yolov8s.yaml').load('../models/yolov8s.pt')  # build from YAML and transfer weights\n",
    "\n",
    "epochs = 25\n",
    "save_dir = '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-bluntv2i-v8s'\n",
    "#data     = '/home/irfan/Desktop/Data/Tools segmentation 2.v3i.yolov8/data.yaml'\n",
    "data     = '/home/irfan/Desktop/Data/blunt.v2i.yolov8/data.yaml'\n",
    "os.system(f'rm -r {save_dir}/*')\n",
    "result_logger = ResultLogger(save_path=f'{save_dir}/log.csv')\n",
    "best_map = -1\n",
    "for epoch in range(epochs):\n",
    "    os.makedirs(f'{save_dir}/{epoch}/train/',exist_ok=True)\n",
    "    os.makedirs(f'{save_dir}/{epoch}/val/',exist_ok=True)\n",
    "    #results = model.train(data='coco128.yaml', epochs=10, imgsz=640)\n",
    "    train_results = model.train(data=data, save_dir= f'{save_dir}/{epoch}/train/', epochs=1, batch=4, imgsz=640)\n",
    "    val_results   = model.val(imgsz=640,batch=4,save_dir= f'{save_dir}/{epoch}/val/')\n",
    "    curr_map      = val_results.box.map50\n",
    "    result_logger.update(model,val_results,epoch)\n",
    "    if curr_map > best_map:\n",
    "        model.export(save_dir=f'{save_dir}/best_{epochs}_{curr_map}.pt',format='onnx')\n",
    "#results = model.train(data='coco128.yaml', epochs=10, imgsz=640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b21aca73-a646-4d6a-8a79-45454b0a2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_pth = '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-v8s/0/train/weights/last.pt'\n",
    "model = YOLO(mdl_pth)#.load(mdl_pth)  # build from YAML and transfer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8492f8e6-740a-4cf0-9453-646f566b2696",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (11): Concat()\n",
       "      (12): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (14): Concat()\n",
       "      (15): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Concat()\n",
       "      (18): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Concat()\n",
       "      (21): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): Detect(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(128, 17, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(128, 17, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(128, 17, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455cc784-5fcd-46e8-b519-1090c6f2513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/logs/exp-v8s/8/best_0.35.torchscript'\n",
    "data       = '/home/irfan/Desktop/Data/Tools segmentation 2.v3i.yolov8/data.yaml'\n",
    "model.load(model_file)\n",
    "val_results   = model.val(data=data, imgsz=640,batch=4,save_dir= f'old/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "996ef827-d704-4b6b-b835-b1985fa1c468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0481559038162231"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainer.tloss[ind].cpu().item()#.numpy()\n",
    "#val_results.__dict__['confusion_matrix'].__dict__['matrix'].shape\n",
    "#dict_keys(['task', 'matrix', 'nc', 'conf', 'iou_thres'])\n",
    "#res.__dict__['p']\n",
    "#dict_keys(['p', 'r', 'f1', 'all_ap', 'ap_class_index', 'nc', 'p_curve', 'r_curve', 'f1_curve', 'px', 'prec_values'])\n",
    "#train_results.box#.__dict__['box']#['box']\n",
    "#dict_keys(['save_dir', 'plot', 'on_plot', 'names', 'box', 'speed', 'task', 'confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3286c405-5d1e-4cb4-adbd-10436cbad559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:00<00:00, 2884.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = TQDM(enumerate(model.trainer.train_loader), total=10)\n",
    "for i,batch in pbar:\n",
    "    print(i)\n",
    "    if i== 5: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-1.12.0",
   "language": "python",
   "name": "torch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
